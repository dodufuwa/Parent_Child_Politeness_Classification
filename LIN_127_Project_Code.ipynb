{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-k5Aa8Z4n9Z"
      },
      "source": [
        "# LIN 127 Final Project Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "WYm_8C16r-AT"
      },
      "source": [
        ">[LIN 127 Final Project Code](#scrollTo=L-k5Aa8Z4n9Z)\n",
        "\n",
        ">>[Imports Datasets from Google Drive to Colab](#scrollTo=xojLqA1sNthH)\n",
        "\n",
        ">>>[Datasets for English](#scrollTo=70pfYO5T4U-z)\n",
        "\n",
        ">>>[Datasets for Spanish](#scrollTo=AKJaIFKv4IYq)\n",
        "\n",
        ">>>[Scripts that Convert Each Annotated Data to Useable CSV Files](#scrollTo=XUH9W1SpLECX)\n",
        "\n",
        ">[PART 1: Classifying Politeness in Mainstream American English](#scrollTo=i3SCaz9fF_8B)\n",
        "\n",
        ">>[North American English Training Set: Calculating Priors](#scrollTo=BWFVb_zrEDgZ)\n",
        "\n",
        ">>[Norther American English Development Set: Testing the Classifier](#scrollTo=BoEwrP4M0csC)\n",
        "\n",
        ">[PART 2: Classifying Politeness in UK English](#scrollTo=pSnsr-eoI1zJ)\n",
        "\n",
        ">[PART 3: Classifying Politeness in Mexican Spanish](#scrollTo=7VbUuT8nftML)\n",
        "\n",
        ">>[MEXICAN SPANISH TRAINING SET: CALCULATING PRIORS](#scrollTo=SgmMu307foiM)\n",
        "\n",
        ">>[CASTILIAN SPANISH DEVELOPMENT SET: TESTING THE CLASSIFIER](#scrollTo=LHnhsDpxhO-l)\n",
        "\n",
        ">[PART 4: Classifying Politeness in Castilian Spanish](#scrollTo=UE47_ugW34g6)\n",
        "\n",
        ">[Frequency Distributions](#scrollTo=NKEAKqfNQvQj)\n",
        "\n",
        ">[PART 5: Classification with LLM](#scrollTo=dWxwdK9F41cb)\n",
        "\n",
        ">>[Classification of Politeness in UKE Using LLM](#scrollTo=7pcmRcl17x-s)\n",
        "\n",
        ">>[Classification of Politeness in Castilian Spanish Using LLM](#scrollTo=cbQ7dMMW73sH)\n",
        "\n",
        ">>[Politeness Terms in Classified Data](#scrollTo=btYh0VKzO9vn)\n",
        "\n",
        ">>[LLM's Markers of Politeness in English and Spanish Training Sets](#scrollTo=P1oVxW4n7Puo)\n",
        "\n",
        ">>[Unused Code](#scrollTo=o1JeKHvEDM9a)\n",
        "\n",
        ">[PART 6: Word Vectors](#scrollTo=Y3Unjg4ZCdnl)\n",
        "\n",
        ">>[Aligned Word Vectors for English](#scrollTo=Pw07bWY-EE7q)\n",
        "\n",
        ">>[Aligned Word Vectors for Spanish](#scrollTo=Jgb1WNb_EDZO)\n",
        "\n",
        ">>[1 Million Most Frequent Types for English and Spanish](#scrollTo=6boKDgNsEQLA)\n",
        "\n",
        ">>>[Top 20 Nearest Spanish Words Given an English Word](#scrollTo=ZoMUNdUF24O1)\n",
        "\n",
        ">>>[Top 20 Nearest English Words Given a Spanish Word](#scrollTo=52EMvHh_2_k0)\n",
        "\n",
        ">[PART 7: Concordance Tables](#scrollTo=ssc01BihDgPP)\n",
        "\n",
        ">[Unused Word Vector Code](#scrollTo=BKsu13DpDBln)\n",
        "\n",
        ">[The End. Thank you!](#scrollTo=7bQT6DVp2g87)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xojLqA1sNthH"
      },
      "source": [
        "## Imports Datasets from Google Drive to Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TvqdFbm6kdv",
        "outputId": "742bd1ba-98f7-47e3-814a-9cdbcab01360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCsqgj5JO4RY"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70pfYO5T4U-z"
      },
      "source": [
        "### Datasets for English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u25fBG44Joi"
      },
      "outputs": [],
      "source": [
        "# childes/Eng-NA/Gleason/Dinner/nanette\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/train_childes_Eng-NA_Gleason_Dinner_nanette.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    trainset_en_na_1 = list(reader)\n",
        "\n",
        "# childes/Eng-NA/Garvey/globob\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/train_childes _ Eng-NA _ Garvey _ globob.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    trainset_en_na_2 = list(reader)\n",
        "\n",
        "# childes/Eng-NA/Bliss/normelis\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/train_childes _ Eng-NA _ Bliss _ normelis.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    trainset_en_na_3 = list(reader)\n",
        "\n",
        "# childes/Eng-NA/Bohannon/Bax/kelley\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/dev_childes _ Eng-NA _ Bohannon _ Bax _ kelley.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    devset_en_na_1 = list(reader)\n",
        "\n",
        "# childes/Eng-UK/Edinburgh/adam1508\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/test_childes _ Eng-UK _ Edinburgh _ adam1508.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    testset_en_uk_1 = list(reader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKJaIFKv4IYq"
      },
      "source": [
        "### Datasets for Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON-9mZrB4HLQ"
      },
      "outputs": [],
      "source": [
        "# childes/Spanish/Montes/020710\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/train_childes_Spanish(México)_montes_020710.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    trainset_es_mx_1 = list(reader)\n",
        "\n",
        "# childes/Spanish/Montes/020914\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/train_childes_Spanish(México)_Montes_020914.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    trainset_es_mx_2 = list(reader)\n",
        "\n",
        "# childes/Spanish/Marrero/Alfonso/021022\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/dev_childes_Spanish(Spain)_marrero_021022.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    devset_es_es_1 = list(reader)\n",
        "\n",
        "# childes/Spanish/Marrero/Rafael/040416\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/test_childes_Spanish(Spain)_marrero_rafael_040416.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    testset_es_es_1 = list(reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjDbCuBTU4t6"
      },
      "outputs": [],
      "source": [
        "# Parent and child tags from each transcript that will be used\n",
        "trainset_en_na_1_tags = ['*MOT:', '*FAT:', '*CHI:']\n",
        "trainset_en_na_2_tags = ['*BOB', '*CHI']\n",
        "trainset_en_na_3_tags = ['*CHI', '*RAN']\n",
        "devset_en_na_1_tags = ['*CHI', '*KEL']\n",
        "testset_en_uk_1_tags = ['*CHI', '*FAT', '*MOT']\n",
        "\n",
        "trainset_es_mx_1_tags = ['CHI', 'MOT', 'JIM']\n",
        "trainset_es_mx_2_tags = ['CHI', 'MOT', 'JIM']\n",
        "devset_es_es_1_tags = ['CHI', 'MOT', 'RAF']\n",
        "testset_es_es_1_tags = ['CHI', 'ALF', 'JOS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUH9W1SpLECX"
      },
      "source": [
        "### Scripts that Convert Each Annotated Data to Useable CSV Files\n",
        "Written with the help of GPT-4o mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzWrE7IvI9T2"
      },
      "outputs": [],
      "source": [
        "# trainset_en_na_1\n",
        "\n",
        "filtered_data = []\n",
        "for row in trainset_en_na_1:\n",
        "    if row[1] in trainset_en_na_1_tags:\n",
        "        filtered_data.append(row[:3])  # Append only the first three columns (Annotations, Speaker, Utterance)\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'trainset_en_na_1.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxDxdQleQc_b"
      },
      "outputs": [],
      "source": [
        "# trainset_en_na_2\n",
        "\n",
        "filtered_data = []\n",
        "for row in trainset_en_na_2:\n",
        "    if row[1] in trainset_en_na_2_tags:\n",
        "        filtered_data.append(row[:3])  # Append only the first three columns (Annotations, Speaker, Utterance)\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'trainset_en_na_2.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiReTg-wQdc0"
      },
      "outputs": [],
      "source": [
        "# trainset_en_na_3\n",
        "\n",
        "filtered_data = []\n",
        "for row in trainset_en_na_3:\n",
        "    if row[1] in trainset_en_na_3_tags:\n",
        "        filtered_data.append(row[:3])  # Append only the first three columns (Annotations, Speaker, Utterance)\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'trainset_en_na_3.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIigLP2oQdle"
      },
      "outputs": [],
      "source": [
        "# devset_en_na_1\n",
        "\n",
        "filtered_data = []\n",
        "for row in devset_en_na_1:\n",
        "    if row[1] in devset_en_na_1_tags:\n",
        "        filtered_data.append(row[:3])  # Append only the first three columns (Annotations, Speaker, Utterance)\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'devset_en_na_1.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpPkZZ9zAWhR"
      },
      "outputs": [],
      "source": [
        "# testset_en_uk_1\n",
        "\n",
        "filtered_data = []\n",
        "for row in testset_en_uk_1:\n",
        "    if row[1] in testset_en_uk_1_tags:\n",
        "        filtered_data.append(row[:3])  # Append only the first three columns (Annotations, Speaker, Utterance)\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'testset_en_uk_1.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UURihfqSkFyz"
      },
      "outputs": [],
      "source": [
        "# trainset_es_mx_1\n",
        "\n",
        "filtered_data = []\n",
        "for row in trainset_es_mx_1:\n",
        "    if row[1].startswith(trainset_es_mx_1_tags[0]) or row[1].startswith(trainset_es_mx_1_tags[1]) or row[1].startswith(trainset_es_mx_1_tags[2]):\n",
        "        tag = row[1][:3]  # Get the first three characters in col 2 of the annotated data\n",
        "        utterance = row[1][4:].strip()\n",
        "        filtered_data.append([row[0], tag, utterance])\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'trainset_es_mx_1.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddvpTgWGQd1-"
      },
      "outputs": [],
      "source": [
        "# trainset_es_mx_2\n",
        "\n",
        "filtered_data = []\n",
        "for row in trainset_es_mx_2:\n",
        "    if row[1].startswith(trainset_es_mx_2_tags[0]) or row[1].startswith(trainset_es_mx_2_tags[1]) or row[1].startswith(trainset_es_mx_2_tags[2]):\n",
        "        tag = row[1][:3]  # Get the first three characters in col 2 of the annotated data\n",
        "        utterance = row[1][4:].strip()\n",
        "        filtered_data.append([row[0], tag, utterance])\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'trainset_es_mx_2.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZRA2YxfQd-c"
      },
      "outputs": [],
      "source": [
        "# devset_es_es_1\n",
        "\n",
        "filtered_data = []\n",
        "for row in devset_es_es_1:\n",
        "    if row[1].startswith(devset_es_es_1_tags[0]) or row[1].startswith(devset_es_es_1_tags[1]) or row[1].startswith(devset_es_es_1_tags[2]):\n",
        "        tag = row[1][:3]  # Get the first three characters in col 2 of the annotated data\n",
        "        utterance = row[1][4:].strip()\n",
        "        filtered_data.append([row[0], tag, utterance])\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'devset_es_es_1.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LhJYvXCQeIH"
      },
      "outputs": [],
      "source": [
        "# testset_es_es_1\n",
        "\n",
        "filtered_data = []\n",
        "for row in testset_es_es_1:\n",
        "    if row[1].startswith(testset_es_es_1_tags[0]) or row[1].startswith(testset_es_es_1_tags[1]) or row[1].startswith(testset_es_es_1_tags[2]):\n",
        "        tag = row[1][:3]  # Get the first three characters in col 2 of the annotated data\n",
        "        utterance = row[1][4:].strip()\n",
        "        filtered_data.append([row[0], tag, utterance])\n",
        "\n",
        "output_file_path = os.path.join(os.getcwd(), 'testset_es_es_1.csv')\n",
        "\n",
        "with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(filtered_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3SCaz9fF_8B"
      },
      "source": [
        "# PART 1: Classifying Politeness in Mainstream American English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWFVb_zrEDgZ"
      },
      "source": [
        "## North American English Training Set: Calculating Priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTf99vhu_vym"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import csv\n",
        "\n",
        "file_names = ['trainset_en_na_1.csv', 'trainset_en_na_2.csv', 'trainset_en_na_3.csv']\n",
        "en_na_train_data = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    with open(file_name, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)\n",
        "        en_na_train_data.extend(list(reader))\n",
        "\n",
        "pol_fd = Counter()\n",
        "nop_fd = Counter()\n",
        "\n",
        "for label in en_na_train_data:\n",
        "  if label[0] == 'POL':\n",
        "    tokens = label[0].split()\n",
        "    pol_fd.update(tokens)\n",
        "  else:\n",
        "    tokens = label[0].split()\n",
        "    nop_fd.update(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH-ehJ3cAUn7",
        "outputId": "3ed31199-b534-4acd-e5a7-757e26d81c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'POL': 47})\n",
            "Counter({'NOP': 1075, 'POl': 1})\n"
          ]
        }
      ],
      "source": [
        "print(pol_fd)\n",
        "print(nop_fd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gNsV5-XGnjs",
        "outputId": "35d2ae0d-aeb4-44b4-fa55-c2a4a9fc0dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.019704433497536946\n",
            "0.9802955665024631\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "pol_fd = Counter()\n",
        "nop_fd = Counter()\n",
        "\n",
        "pol_count = 0\n",
        "nop_count = 0\n",
        "\n",
        "en_na_train_data = ['trainset_en_na_1', 'trainset_en_na_2', 'trainset_en_na_3']\n",
        "\n",
        "for file_data in en_na_train_data:\n",
        "    with open(file_name, 'r') as file:\n",
        "      reader = csv.reader(file)\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        label = row[0]\n",
        "        text = row[2]\n",
        "        if label == 'POL':\n",
        "           pol_count += 1\n",
        "           tokens = text.split()\n",
        "           pol_fd.update(tokens)\n",
        "        elif label == 'NOP':\n",
        "           nop_count += 1\n",
        "           tokens = text.split()\n",
        "           nop_fd.update(tokens)\n",
        "\n",
        "total_lines = pol_count + nop_count\n",
        "pol_prior = pol_count / total_lines\n",
        "nop_prior = nop_count / total_lines\n",
        "\n",
        "print(pol_prior)\n",
        "print(nop_prior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6EIlDaa9Xr2"
      },
      "outputs": [],
      "source": [
        "for row in en_na_train_data:\n",
        "  if row[0] == 'POL':\n",
        "    pol_count += 1\n",
        "    tokens = row[1].split()\n",
        "    pol_fd.update(tokens)\n",
        "  else:\n",
        "    nop_count += 1\n",
        "    tokens = row[1].split()\n",
        "    nop_fd.update(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ABtL7AelcKV"
      },
      "outputs": [],
      "source": [
        "trainset_en_na_pol_fd = pol_fd\n",
        "trainset_en_na_nop_fd = nop_fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoEwrP4M0csC"
      },
      "source": [
        "## Norther American English Development Set: Testing the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohcQ89pE0bjG"
      },
      "outputs": [],
      "source": [
        "fp = open('devset_en_na_1.csv')\n",
        "reader = csv.reader(fp)\n",
        "en_na_dev_data = list(reader)\n",
        "fp.close()\n",
        "\n",
        "tokens = en_na_dev_data\n",
        "\n",
        "tokens = en_na_dev_data[0][1].split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af7v8-rLtfHY"
      },
      "outputs": [],
      "source": [
        "fp = open('devset_en_na_1.csv')\n",
        "reader = csv.reader(fp)\n",
        "en_na_dev_data = list(reader)\n",
        "fp.close()\n",
        "\n",
        "tokens = en_na_dev_data\n",
        "\n",
        "tokens = en_na_dev_data[0][1].split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hPlbLoMvtvJm",
        "outputId": "5405040f-02e1-4275-ab1d-2b8a71b3b8f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this one ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "en_na_dev_data[99][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VviEqwuxGHT6",
        "outputId": "06de5e74-8d43-4bfc-b456-39a938ec9826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9568034557235421\n"
          ]
        }
      ],
      "source": [
        "from math import log\n",
        "from collections import Counter\n",
        "\n",
        "correct_predictions = 0\n",
        "total_lines = len(en_na_dev_data)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "total_fd = Counter()\n",
        "\n",
        "for label, speaker, text in en_na_dev_data:\n",
        "  tokens = text.split()\n",
        "  total_fd.update(tokens)\n",
        "\n",
        "vocab_size = (len(total_fd))\n",
        "\n",
        "total_pol_tokens = 0\n",
        "total_nop_tokens = 0\n",
        "\n",
        "for label, speaker, text in en_na_dev_data:\n",
        "  if label == 'POL':\n",
        "    tokens = text.split()\n",
        "    total_pol_tokens += len(tokens)\n",
        "  elif label == 'NOP':\n",
        "    tokens = text.split()\n",
        "    total_nop_tokens\n",
        "\n",
        "for label, speaker, text in en_na_dev_data:\n",
        "  tokens = text.split()\n",
        "  pol_score = log(pol_prior)\n",
        "  nop_score = log(nop_prior)\n",
        "  for token in tokens:\n",
        "    pol_count = pol_fd.get(token,0)+1\n",
        "    pol_score += log(pol_count + 1/(total_pol_tokens + vocab_size + 1))\n",
        "    nop_count = nop_fd.get(token,0)+1\n",
        "    nop_score += log(nop_count + 1/(total_nop_tokens + vocab_size + 1))\n",
        "\n",
        "  predicted_label = 'POL' if pol_score > nop_score else 'NOP'\n",
        "  if predicted_label == label:\n",
        "    correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_lines\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIl4NA9qunEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSnsr-eoI1zJ"
      },
      "source": [
        "# PART 2: Classifying Politeness in UK English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jQwFxRWI1V_"
      },
      "outputs": [],
      "source": [
        "testset_en_uk_1_output = []\n",
        "\n",
        "import csv\n",
        "fp = open('testset_en_uk_1.csv')\n",
        "reader = csv.reader(fp)\n",
        "testset_en_uk_1 = list(reader)\n",
        "fp.close()\n",
        "\n",
        "from math import log\n",
        "from collections import Counter\n",
        "\n",
        "correct_predictions = 0\n",
        "total_lines = len(testset_en_uk_1)\n",
        "\n",
        "from collections import Counter\n",
        "total_fd = Counter()\n",
        "\n",
        "for review in testset_en_uk_1:\n",
        "    tokens = review[1].split()\n",
        "    total_fd.update(tokens)\n",
        "\n",
        "vocab_size = (len(total_fd))\n",
        "\n",
        "total_pol_tokens = 0\n",
        "total_nop_tokens = 0\n",
        "\n",
        "for label, speaker, text in en_na_dev_data:\n",
        "  if label == 'POL':\n",
        "    tokens = text.split()\n",
        "    total_pol_tokens += len(tokens)\n",
        "  elif label == 'NOP':\n",
        "    tokens = text.split()\n",
        "    total_nop_tokens\n",
        "\n",
        "for label, speaker, text in testset_en_uk_1:\n",
        "  tokens = text.split()\n",
        "  pol_score = log(pol_prior)\n",
        "  nop_score = log(nop_prior)\n",
        "  for token in tokens:\n",
        "    pol_count = pol_fd.get(token,0)+1\n",
        "    pol_score += log(pol_count/(total_pol_tokens + vocab_size))\n",
        "    nop_count = nop_fd.get(token,0)+1\n",
        "    nop_score += log(nop_count/(total_nop_tokens + vocab_size))\n",
        "\n",
        "  predicted_label = 'POL' if pol_score > nop_score else 'NOP'\n",
        "  predicted_label = 'POL'\n",
        "  if nop_score > pol_score:\n",
        "    predicted_label = 'NOP'\n",
        "\n",
        "  testset_en_uk_1_output.append(predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr2N4zWNJfIZ",
        "outputId": "aeff09ea-0a1f-418b-f3d4-d623e9d65ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "647\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(testset_en_uk_1_output)\n",
        "print(label_counts[\"NOP\"])\n",
        "print(label_counts[\"POL\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VbUuT8nftML"
      },
      "source": [
        "# PART 3: Classifying Politeness in Mexican Spanish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgmMu307foiM"
      },
      "source": [
        "## MEXICAN SPANISH TRAINING SET: CALCULATING PRIORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuw9NPx8gUKS"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import csv\n",
        "\n",
        "file_names = ['trainset_es_mx_1.csv', 'trainset_es_mx_2.csv']\n",
        "es_mx_train_data = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    with open(file_name, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)\n",
        "        es_mx_train_data.extend(list(reader))\n",
        "\n",
        "pol_fd = Counter()\n",
        "nop_fd = Counter()\n",
        "\n",
        "for label in es_mx_train_data:\n",
        "  if label[0] == 'POL':\n",
        "    tokens = label[0].split()\n",
        "    pol_fd.update(tokens)\n",
        "  else:\n",
        "    tokens = label[0].split()\n",
        "    nop_fd.update(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqFAEPTygvA5",
        "outputId": "1e9dd266-c1dd-4670-def9-633850743d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'POL': 207})\n",
            "Counter({'NOP': 862, 'XXX': 72, 'IMP': 1, 'BOP': 1})\n"
          ]
        }
      ],
      "source": [
        "print(pol_fd)\n",
        "print(nop_fd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmhanyM2gzRQ",
        "outputId": "7e17e89a-080e-441c-f98a-e35178165638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16279069767441862\n",
            "0.8372093023255814\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "pol_fd = Counter()\n",
        "nop_fd = Counter()\n",
        "\n",
        "pol_count = 0\n",
        "nop_count = 0\n",
        "\n",
        "es_mx_train_data = ['trainset_es_mx_1', 'trainset_es_mx_2']\n",
        "\n",
        "for file_data in es_mx_train_data:\n",
        "    with open(file_name, 'r') as file:\n",
        "      reader = csv.reader(file)\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        label = row[0]\n",
        "        text = row[2]\n",
        "        if label == 'POL':\n",
        "           pol_count += 1\n",
        "           tokens = text.split()\n",
        "           pol_fd.update(tokens)\n",
        "        elif label == 'NOP':\n",
        "           nop_count += 1\n",
        "           tokens = text.split()\n",
        "           nop_fd.update(tokens)\n",
        "\n",
        "total_lines = pol_count + nop_count\n",
        "pol_prior = pol_count / total_lines\n",
        "nop_prior = nop_count / total_lines\n",
        "\n",
        "print(pol_prior)\n",
        "print(nop_prior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njk79iiNhHYp"
      },
      "outputs": [],
      "source": [
        "for row in es_mx_train_data:\n",
        "  if row[0] == 'POL':\n",
        "    pol_count += 1\n",
        "    tokens = row[1].split()\n",
        "    pol_fd.update(tokens)\n",
        "  else:\n",
        "    nop_count += 1\n",
        "    tokens = row[1].split()\n",
        "    nop_fd.update(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz0ppUpPqJqE"
      },
      "outputs": [],
      "source": [
        "trainset_es_mx_pol_fd = pol_fd\n",
        "trainset_es_mx_nop_fd = nop_fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHnhsDpxhO-l"
      },
      "source": [
        "## CASTILIAN SPANISH DEVELOPMENT SET: TESTING THE CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYhoCWVahzj5"
      },
      "outputs": [],
      "source": [
        "fp = open('devset_es_es_1.csv')\n",
        "reader = csv.reader(fp)\n",
        "es_es_dev_data = list(reader)\n",
        "fp.close()\n",
        "\n",
        "tokens = es_es_dev_data\n",
        "\n",
        "tokens = es_es_dev_data[0][1].split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wibfLNdmh4ks"
      },
      "outputs": [],
      "source": [
        "fp = open('devset_es_es_1.csv')\n",
        "reader = csv.reader(fp)\n",
        "es_es_dev_data = list(reader)\n",
        "fp.close()\n",
        "\n",
        "tokens = es_es_dev_data\n",
        "\n",
        "tokens = es_es_dev_data[0][1].split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1E1PX2IiTSW",
        "outputId": "647be922-9059-4b1f-e49d-5003e91405cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'qué ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "es_es_dev_data[99][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUubs13biZuK",
        "outputId": "a921be69-00ab-4fbd-964b-04af64c67914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8554216867469879\n"
          ]
        }
      ],
      "source": [
        "from math import log\n",
        "from collections import Counter\n",
        "\n",
        "correct_predictions = 0\n",
        "total_lines = len(es_es_dev_data)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "total_fd = Counter()\n",
        "\n",
        "for label, speaker, text in es_es_dev_data:\n",
        "  tokens = text.split()\n",
        "  total_fd.update(tokens)\n",
        "\n",
        "vocab_size = (len(total_fd))\n",
        "\n",
        "total_pol_tokens = 0\n",
        "total_nop_tokens = 0\n",
        "\n",
        "for label, speaker, text in es_es_dev_data:\n",
        "  if label == 'POL':\n",
        "    tokens = text.split()\n",
        "    total_pol_tokens += len(tokens)\n",
        "  elif label == 'NOP':\n",
        "    tokens = text.split()\n",
        "    total_nop_tokens\n",
        "\n",
        "for label, speaker, text in es_es_dev_data:\n",
        "  tokens = text.split()\n",
        "  pol_score = log(pol_prior)\n",
        "  nop_score = log(nop_prior)\n",
        "  for token in tokens:\n",
        "    pol_count = pol_fd.get(token,0)+1\n",
        "    pol_score += log(pol_count + 1/(total_pol_tokens + vocab_size + 1))\n",
        "    nop_count = nop_fd.get(token,0)+1\n",
        "    nop_score += log(nop_count + 1/(total_nop_tokens + vocab_size + 1))\n",
        "\n",
        "  predicted_label = 'POL' if pol_score > nop_score else 'NOP'\n",
        "  if predicted_label == label:\n",
        "    correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_lines\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(row[0] for row in es_es_dev_data)\n",
        "print(label_counts[\"NOP\"])\n",
        "print(label_counts[\"POL\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBqPQ_AyGGLr",
        "outputId": "d82b5365-99c1-4aa5-e651-ddf50a460976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "283\n",
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE47_ugW34g6"
      },
      "source": [
        "# PART 4: Classifying Politeness in Castilian Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhFv-RREi3Jg"
      },
      "outputs": [],
      "source": [
        "\n",
        "testset_es_es_1_output = []\n",
        "\n",
        "import csv\n",
        "fp = open('testset_es_es_1.csv')\n",
        "reader = csv.reader(fp)\n",
        "testset_es_es_1 = list(reader)\n",
        "fp.close()\n",
        "\n",
        "from math import log\n",
        "from collections import Counter\n",
        "\n",
        "correct_predictions = 0\n",
        "total_lines = len(testset_es_es_1)\n",
        "\n",
        "from collections import Counter\n",
        "total_fd = Counter()\n",
        "\n",
        "for review in testset_es_es_1:\n",
        "    tokens = review[1].split()\n",
        "    total_fd.update(tokens)\n",
        "\n",
        "vocab_size = (len(total_fd))\n",
        "\n",
        "total_pol_tokens = 0\n",
        "total_nop_tokens = 0\n",
        "\n",
        "for label, speaker, text in es_es_dev_data:\n",
        "  if label == 'POL':\n",
        "    tokens = text.split()\n",
        "    total_pol_tokens += len(tokens)\n",
        "  elif label == 'NOP':\n",
        "    tokens = text.split()\n",
        "    total_nop_tokens\n",
        "\n",
        "for label, speaker, text in testset_es_es_1:\n",
        "  tokens = text.split()\n",
        "  pol_score = log(pol_prior)\n",
        "  nop_score = log(nop_prior)\n",
        "  for token in tokens:\n",
        "    pol_count = pol_fd.get(token,0)+1\n",
        "    pol_score += log(pol_count/(total_pol_tokens + vocab_size))\n",
        "    nop_count = nop_fd.get(token,0)+1\n",
        "    nop_score += log(nop_count/(total_nop_tokens + vocab_size))\n",
        "\n",
        "  predicted_label = 'POL' if pol_score > nop_score else 'NOP'\n",
        "  predicted_label = 'POL'\n",
        "  if nop_score > pol_score:\n",
        "    predicted_label = 'NOP'\n",
        "\n",
        "  testset_es_es_1_output.append(predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL_eDhL4XQ_N",
        "outputId": "166e5063-8ef8-4dfe-cb00-95e8c483b9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "563\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(testset_es_es_1_output)\n",
        "print(label_counts[\"NOP\"])\n",
        "print(label_counts[\"POL\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKEAKqfNQvQj"
      },
      "source": [
        "# Frequency Distributions\n",
        "(for most common words in the training sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfp67YxXlqKW",
        "outputId": "b4289d0c-e425-4cd1-f6b4-b4b278c6af52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 12),\n",
              " ('?', 9),\n",
              " ('could', 6),\n",
              " ('see', 6),\n",
              " (\"don't\", 3),\n",
              " ('know', 3),\n",
              " ('can', 3),\n",
              " ('.', 3),\n",
              " (\"I'll\", 3),\n",
              " ('put', 3),\n",
              " ('it', 3),\n",
              " ('on', 3),\n",
              " ('(o)kay', 3),\n",
              " ('the', 3),\n",
              " ('Christmas', 3),\n",
              " ('trees', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "trainset_en_na_pol_fd.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8xshwT8pgYB",
        "outputId": "7c8ae216-5e3c-4169-f289-8726caa9fa75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 396),\n",
              " ('?', 192),\n",
              " ('the', 87),\n",
              " ('you', 66),\n",
              " ('this', 66),\n",
              " ('[//]', 54),\n",
              " ('I', 54),\n",
              " ('a', 51),\n",
              " ('(.)', 51),\n",
              " ('gonna', 48),\n",
              " ('she', 48),\n",
              " ('on', 39),\n",
              " ('to', 39),\n",
              " ('it', 36),\n",
              " ('at', 36),\n",
              " ('right', 36),\n",
              " ('(o)kay', 36),\n",
              " ('here', 36),\n",
              " ('gotta', 36),\n",
              " ('one', 36)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "trainset_en_na_nop_fd.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc-RMdtVq5j2",
        "outputId": "8a7fd1c3-4c3b-4cea-8147-8fc413828ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('?', 94),\n",
              " ('.', 76),\n",
              " ('qué', 54),\n",
              " ('usted', 26),\n",
              " ('mamá', 26),\n",
              " ('la', 26),\n",
              " ('[?]', 22),\n",
              " ('y', 20),\n",
              " ('bueno', 20),\n",
              " ('cómo', 20),\n",
              " ('se', 16),\n",
              " ('a', 16),\n",
              " ('no', 16),\n",
              " ('está', 14),\n",
              " ('[/]', 12),\n",
              " ('dónde', 12),\n",
              " ('las', 12),\n",
              " (',', 10),\n",
              " ('de', 10),\n",
              " ('[//]', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "trainset_es_mx_pol_fd.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX42twKDq8CE",
        "outputId": "b872bc81-93b4-4b8e-f11d-0fc015f583aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 676),\n",
              " ('no', 162),\n",
              " ('?', 160),\n",
              " ('[?]', 116),\n",
              " ('a', 82),\n",
              " ('la', 80),\n",
              " ('!', 76),\n",
              " ('se', 72),\n",
              " ('el', 72),\n",
              " ('y', 70),\n",
              " ('es', 68),\n",
              " ('un', 62),\n",
              " ('me', 60),\n",
              " ('de', 60),\n",
              " ('voy', 52),\n",
              " (',', 52),\n",
              " ('sí', 50),\n",
              " ('con', 44),\n",
              " ('lo', 42),\n",
              " ('está', 40)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "trainset_es_mx_nop_fd.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWxwdK9F41cb"
      },
      "source": [
        "# PART 5: Classification with LLM\n",
        "(Llama-3.1 8b from HW4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOpFF2aTQvBH",
        "outputId": "d7833b73-0a54-4f2c-fa62-adbdaab94fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 01:32:12--  https://compling.ucdavis.edu/LIN127/hw4.py\n",
            "Resolving compling.ucdavis.edu (compling.ucdavis.edu)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to compling.ucdavis.edu (compling.ucdavis.edu)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1420 (1.4K) [application/octet-stream]\n",
            "Saving to: ‘hw4.py’\n",
            "\n",
            "hw4.py              100%[===================>]   1.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-07 01:32:12 (22.5 MB/s) - ‘hw4.py’ saved [1420/1420]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://compling.ucdavis.edu/LIN127/hw4.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQQcGT9hXBEu",
        "outputId": "4a4148e7-4c98-4bda-c83b-cb951570a6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and installing requirements...\n",
            "Starting...\n",
            "Downloading model: llama3.1:8b\n",
            "Preparing model...\n",
            "Ready\n"
          ]
        }
      ],
      "source": [
        "import hw4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pcmRcl17x-s"
      },
      "source": [
        "## Classification of Politeness in UKE Using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAfWgQMh6HqC"
      },
      "outputs": [],
      "source": [
        "hw4.system_msg = 'You are a politeness classifier. The user will provide an utterance (in UKE English) said by a child or an adult in conversation, and you will output simple \"POL\" if the utterance is polite, or \"NOP\" if the utterance is impolite (and nothing else).'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38UIJTlh56vi"
      },
      "outputs": [],
      "source": [
        "history = ['hey hey Katie please don\\'t do that .',\n",
        "           'POL',\n",
        "           'I mean don''t tickle me please .'\n",
        "           'POL',\n",
        "           'hey Rusty has that hat .'\n",
        "           'NOP',\n",
        "           'now I\\'ll wear cowboy hat you wanna be a cowboy ?'\n",
        "           'NOP',\n",
        "           'okay , would you like to try some of this ?',\n",
        "           'POL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad5otRSe41FT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd183f5-8ccf-48a8-e770-0aef4d911795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines classified: 1\n",
            "Number of lines classified: 2\n",
            "Number of lines classified: 3\n",
            "Number of lines classified: 4\n",
            "Number of lines classified: 5\n",
            "Number of lines classified: 6\n",
            "Number of lines classified: 7\n",
            "Number of lines classified: 8\n",
            "Number of lines classified: 9\n",
            "Number of lines classified: 10\n",
            "Final accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for r in devset_en_na_1:\n",
        "  output = hw4.gen(r[2], history) # r[2] is the utterance\n",
        "  output = output.strip()\n",
        "  print(output)\n",
        "\n",
        "  if output == r[0]:\n",
        "    correct += 1\n",
        "\n",
        "  total += 1\n",
        "\n",
        "# print('Final accuracy:', correct / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbQ7dMMW73sH"
      },
      "source": [
        "## Classification of Politeness in Castilian Spanish Using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2gNNr4X6oZ7"
      },
      "outputs": [],
      "source": [
        "hw4.system_msg = 'You are a politeness classifier. The user will provide an utterance (in Castilian Spanish) said by a child or an adult in conversation, and you will output simple \"POL\" if the utterance is polite, or \"NOP\" if the utterance is impolite (and nothing else).'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcE5N67K7ju_"
      },
      "outputs": [],
      "source": [
        "history = ['a_ver , enséñame las pinturas , dónde están ?',\n",
        "'POL',\n",
        "'uy , qué bien .',\n",
        "'POL',\n",
        "'dónde tienes el sacapuntas ?',\n",
        "'POL',\n",
        "'vale , sácale punta .',\n",
        "'POL',\n",
        "'vale .',\n",
        "'POL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBrfir8A5iXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39385a9d-33ab-4647-8df2-6ce779665e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines classified: 1\n",
            "Number of lines classified: 2\n",
            "Number of lines classified: 3\n",
            "Number of lines classified: 4\n",
            "Number of lines classified: 5\n",
            "Number of lines classified: 6\n",
            "Number of lines classified: 7\n",
            "Number of lines classified: 8\n",
            "Number of lines classified: 9\n",
            "Number of lines classified: 10\n",
            "Final accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for r in devset_es_es_1:\n",
        "  # Check if the row has enough elements before accessing index 2\n",
        "  if len(r) > 2:\n",
        "    output = hw4.gen(r[2], history) # r[2] is the utterance\n",
        "    output = output.strip()\n",
        "    print(output)\n",
        "\n",
        "    if output == r[0]:\n",
        "      correct += 1\n",
        "\n",
        "  total += 1\n",
        "\n",
        "# print('Final accuracy:', correct / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btYh0VKzO9vn"
      },
      "source": [
        "## Politeness Terms in Classified Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-d4TkTGXQN1"
      },
      "outputs": [],
      "source": [
        "hw4.system_msg = '''You will produce a list of words or phrases in each utterance (said by a parent or child in conversation) that are markers of politeness in North American English. The user will provide a text sample and you will produce the word or phrases that denote politeness for each statement as output.\n",
        "\n",
        "For example:\n",
        "user: I really appreciate that you’ve done them\n",
        "you: I, appreciate\n",
        "user: Nice work so far on your rewrite.\n",
        "you: Nice work\n",
        "user: Wow! / This is a great way to deal. . .\n",
        "you: wow, great\n",
        "\n",
        "Here is some background. The following are types of utterances that are perceived as being polite along with their example:\n",
        "\n",
        "Gratitude 'I really appreciate that you’ve done them.'\n",
        "Deference 'Nice work so far on your rewrite.'\n",
        "Greeting 'Hey, I just tried to . . .'\n",
        "Positive lexicon 'Wow! / This is a great way to deal. . .'\n",
        "Apologizing 'Sorry to bother you . . .'\n",
        "Please 'Could you please say more. . .'\n",
        "Indirect (btw) 'By the way, where did you find . . .'\n",
        "Counterfactual modal 'Could/Would you . . .'\n",
        "Indicative modal 'Can/Will you . . .'\n",
        "1st person start 'I have just put the article . . .'\n",
        "1st person pl. 'Could we find a less complex name . . .'\n",
        "1st person 'It is my view that ...'\n",
        "2nd person 'But what’s the good source you have in mind?'\n",
        "Hedges 'I suggest we start with . . .'\n",
        "\n",
        "Here is additional info. The following are types of utterances that are perceived as being impolite along with their example and the words or phrase that make them impolite:\n",
        "\n",
        "Negative lexicon 'If you’re going to accuse me . . .'\n",
        "Please start 'Please do not remove warnings . . .'\n",
        "Direct question 'What is your native language?'\n",
        "Direct start 'So can you retrieve it or not?'\n",
        "2nd person start 'You’ve reverted yourself . . .'\n",
        "Factuality 'In fact you did link, . . .'\n",
        "\n",
        "In each request, please provide the exact word(s) or phrases that make the given utterance polite. Do not add any other info, comments, or explanation. Do not respond in a conversational way\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI59499MXRD3"
      },
      "outputs": [],
      "source": [
        "# From Pg 3 of https://arxiv.org/pdf/1306.6078\n",
        "# Removed strategies perceived as not being polite\n",
        "\n",
        "examples = ['I really appreciate that you’ve done them',\n",
        "            'I, appreciate',\n",
        "            'Nice work so far on your rewrite.',\n",
        "            'Nice work',\n",
        "            'Hey, I just tried to . . .',\n",
        "            'Hey',\n",
        "            'Wow! / This is a great way to deal. . .',\n",
        "            'Wow, great',\n",
        "            'Sorry to bother you . . .',\n",
        "            'Sorry',\n",
        "            'Could you please say more. . .',\n",
        "            'please',\n",
        "            'By the way, where did you find . . .',\n",
        "            'By the way',\n",
        "            'What is your native language?',\n",
        "            'What',\n",
        "            'So can you retrieve it or not?',\n",
        "            'So',\n",
        "            'Could/Would you . . .',\n",
        "            'Could/Would',\n",
        "            'Can/Will you . . .',\n",
        "            'Can/Will',\n",
        "            'I have just put the article . . .',\n",
        "            'I',\n",
        "            'Could we find a less complex name . . .'\n",
        "            'we',\n",
        "            'It is my view that ...',\n",
        "            'my',\n",
        "            'But what’s the good source you have in mind?',\n",
        "            'you',\n",
        "            'I suggest we start with . .',\n",
        "            'suggest']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCR8KH5p9N1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f57fd09-ebcb-444f-b9c6-9e5d7858a745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if, you\n",
            "(I couldn't find any markers of politeness, but there is a hedge \"carefully\") \n",
            "\n",
            "carefully\n",
            "would you like to try some of this\n",
            "honey\n",
            "(maybe a)\n",
            "sweetheart\n"
          ]
        }
      ],
      "source": [
        "for line in trainset_en_na_1[:500]:\n",
        "  if line[0] == 'POL':\n",
        "    print(hw4.gen(line[2], examples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4fb84nBRGRT"
      },
      "outputs": [],
      "source": [
        "for line in es_mx_train_data[:500]:\n",
        "  if line[0] == 'POL':\n",
        "    print(hw4.gen(line[2], examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1oVxW4n7Puo"
      },
      "source": [
        "## LLM's Markers of Politeness in English and Spanish Training Sets\n",
        "\n",
        "attempt to get frequencies for the word/phrases the LM generated using the frequency distributions of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rumcha9BWzE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e773250-9089-4d3f-f6cd-c94a95e5dd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. thank you\n",
            "2. I appreciate\n",
            "3. Nice work\n",
            "4. wow, great\n",
            "5. Hey\n",
            "6. Could you please\n",
            "7. But what's the good\n",
            "8. That's very kind\n",
            "9. I'm so grateful\n",
            "10. You're welcome\n",
            "11. Thank you so much\n",
            "12. It was my pleasure\n",
            "13. I would love to\n",
            "14. Please say more\n",
            "15. Sorry to bother\n",
            "16. Excuse me\n",
            "17. May I ask\n",
            "18. I think that's a great idea\n",
            "19. That makes sense\n",
            "20. Can we\n"
          ]
        }
      ],
      "source": [
        "output = hw4.gen('Produce an ordered numbered list of 20 words and 2-3 word phrases that may frequently be used in conversation between a child and parent as markers of politeness in North American English (Separate by newlines. Do not add any other info or comments. Do not respond in a conversational way).')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k-mbV75P-yG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c766e8f-cfe8-4933-e58c-068dd3280d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. please\n",
            "2. thank you\n",
            "3. excuse me\n",
            "4. sorry to bother\n",
            "5. I really appreciate\n",
            "6. could you please\n",
            "7. it's so kind of you\n",
            "8. would you mind\n",
            "9. can I ask\n",
            "10. may I\n",
            "11. do you think\n",
            "12. if you don't mind\n",
            "13. could we\n",
            "14. let me know\n",
            "15. would you like\n",
            "16. here, take this\n",
            "17. have a look at\n",
            "18. can I help\n",
            "19. it's really nice of you\n",
            "20. thanks so much\n"
          ]
        }
      ],
      "source": [
        "output = hw4.gen('Produce an ordered numbered list of 20 words and 2-3 word phrases that may frequently be used in conversation between a child and parent as markers of politeness in UK English (Separate by newlines. Do not add any other info or comments. Do not respond in a conversational way).')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wLzG_qqP_TW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ff3a90-343c-41ac-9d3d-82573f325f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Gracias\n",
            "2. Muchas gracias\n",
            "3. Lo siento\n",
            "4. Por favor\n",
            "5. Con permiso\n",
            "6. Disculpa\n",
            "7. ¡Vaya!\n",
            "8. Buen trabajo\n",
            "9. Estoy agradecido\n",
            "10. Me parece bien\n",
            "11. No hay problema\n",
            "12. ¡Genial!\n",
            "13. Excelente idea\n",
            "14. ¿Puedes ayudarme?\n",
            "15. Permítame\n",
            "16. Con gusto\n",
            "17. Agradezco\n",
            "18. Lo agradezco\n",
            "19. Gracias por todo\n",
            "20. Me alegra\n"
          ]
        }
      ],
      "source": [
        "output = hw4.gen('Produce a ordered numbered list of 20 words and 2-3 word phrases that may frequently be used in conversation between a child and parent as markers of politeness in Mexican Spanish (Separate by newlines. Do not add any other info or comments. Do not respond in a conversational way).')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljk6lu9yP_gG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d99fde-2ceb-48bc-f18a-2fd36786d63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Bueno\n",
            "2. Por favor\n",
            "3. Gracias\n",
            "4. Lo siento\n",
            "5. Me disculpa\n",
            "6. ¿Puedes?\n",
            "7. ¿Podrías?\n",
            "8. ¡Genial!\n",
            "9. ¡Muy bien hecho!\n",
            "10. Esto es genial\n",
            "11. ¡Qué idea!\n",
            "12. Qué bueno\n",
            "13. Muy amable\n",
            "14. ¡Excelente trabajo!\n",
            "15. Me parece muy bien\n",
            "16. Estoy muy agradecido/a\n",
            "17. Lo dejo en tus manos\n",
            "18. ¿Te importaría?\n",
            "19. Por lo que más quieras\n",
            "20. A tu disposición\n"
          ]
        }
      ],
      "source": [
        "output = hw4.gen('Produce an ordered numbered list of 20 words and 2-3 word phrases that may frequently be used in conversation between a child and parent as markers of politeness in Castilian Spanish (Separate by newlines. Do not add any other info or comments. Do not respond in a conversational way).')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unused Code"
      ],
      "metadata": {
        "id": "o1JeKHvEDM9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_na_list = [\"please\",\n",
        "              \"could\",\n",
        "              \"thank you\",\n",
        "              \"thanks\"]\n",
        "\n",
        "most_common_words = trainset_en_na_pol_fd.most_common()\n",
        "\n",
        "# Convert to a dictionary if needed (optional, as `Counter` already behaves like a dictionary)\n",
        "word_count = dict(most_common_words)\n",
        "\n",
        "# Function to get count of any specified word\n",
        "def get_word_count(word):\n",
        "    return trainset_en_na_pol_fd.get(word, 0)  # Using the Counter directly\n",
        "\n",
        "# Example word to check\n",
        "word_counts = {}\n",
        "\n",
        "# Iterate through the list of words and get their counts\n",
        "for word in en_na_list:\n",
        "    word_counts[word] = trainset_en_na_pol_fd.get(word, 0)  # Get the count of each word, defaulting to 0 if not found\n",
        "\n",
        "# Print the results\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"The word '{word}' appears {count} times.\")\n",
        "\n",
        "\n",
        "\n",
        "# Get the count of the specified word\n",
        "count = get_word_count(word)\n",
        "print(f\"The word '{word}' appears {count} times.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QUb1B_d9CAE",
        "outputId": "3ddf4331-7f35-48e1-9cf9-b4f9c52ce417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'please' appears 0 times.\n",
            "The word 'could' appears 6 times.\n",
            "The word 'thank you' appears 0 times.\n",
            "The word 'thanks' appears 0 times.\n",
            "The word 'thanks' appears 0 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEETSozbT0Be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "0387ba4a-0b7f-416c-a318-b7cdbc10ceb0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "dictionary update sequence element #0 has length 6; 2 is required",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-422b0230afc6>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_na_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mword_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_uk_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmama_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 6; 2 is required"
          ]
        }
      ],
      "source": [
        "en_uk_list = [ \"please\",\n",
        "    \"thank you\",\n",
        "    \"excuse me\",\n",
        "    \"sorry to bother\",\n",
        "    \"I really appreciate\",\n",
        "    \"could you please\",\n",
        "    \"it's so kind of you\",\n",
        "    \"would you mind\",\n",
        "    \"can I ask\",\n",
        "    \"may I\",\n",
        "    \"do you think\",\n",
        "    \"if you don't mind\",\n",
        "    \"could we\",\n",
        "    \"let me know\",\n",
        "    \"would you like\",\n",
        "    \"here, take this\",\n",
        "    \"have a look at\",\n",
        "    \"can I help\",\n",
        "    \"it's really nice of you\",\n",
        "    \"thanks so much\"]\n",
        "\n",
        "\n",
        "for word in en_na_list:\n",
        "  word_frequency = dict(en_uk_list).get(word, 0)\n",
        "  print(word, mama_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12OdXxS9T0Pi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0f04153c-1020-4bf4-fc71-860a5d98dcaa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainset_en_na_1_pol_fd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-db632711ef83>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"Me alegra\"]\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfreq_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainset_en_na_1_pol_fd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_na_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainset_en_na_1_pol_fd' is not defined"
          ]
        }
      ],
      "source": [
        "es_mx_list = [ \"Gracias\",\n",
        "    \"Muchas gracias\",\n",
        "    \"Lo siento\",\n",
        "    \"Por favor\",\n",
        "    \"Con permiso\",\n",
        "    \"Disculpa\",\n",
        "    \"¡Vaya!\",\n",
        "    \"Buen trabajo\",\n",
        "    \"Estoy agradecido\",\n",
        "    \"Me parece bien\",\n",
        "    \"No hay problema\",\n",
        "    \"¡Genial!\",\n",
        "    \"Excelente idea\",\n",
        "    \"¿Puedes ayudarme?\",\n",
        "    \"Permítame\",\n",
        "    \"Con gusto\",\n",
        "    \"Agradezco\",\n",
        "    \"Lo agradezco\",\n",
        "    \"Gracias por todo\",\n",
        "    \"Me alegra\"]\n",
        "\n",
        "freq_dist = trainset_es_mx_1_pol_fd.most_common()\n",
        "\n",
        "for word in en_na_list:\n",
        "  word_frequency = dict(x).get(word, 0)\n",
        "  print(word, mama_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_mx_list = [\"gracias\",\n",
        "              \"vale\",\n",
        "              \"b\",\n",
        "              \"thanks\"]\n",
        "\n",
        "most_common_words = trainset_en_na_pol_fd.most_common()\n",
        "\n",
        "# Convert to a dictionary if needed (optional, as `Counter` already behaves like a dictionary)\n",
        "word_count = dict(most_common_words)\n",
        "\n",
        "# Function to get count of any specified word\n",
        "def get_word_count(word):\n",
        "    return trainset_en_na_pol_fd.get(word, 0)  # Using the Counter directly\n",
        "\n",
        "# Example word to check\n",
        "word_counts = {}\n",
        "\n",
        "# Iterate through the list of words and get their counts\n",
        "for word in en_na_list:\n",
        "    word_counts[word] = trainset_en_na_pol_fd.get(word, 0)  # Get the count of each word, defaulting to 0 if not found\n",
        "\n",
        "# Print the results\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"The word '{word}' appears {count} times.\")\n",
        "\n",
        "\n",
        "\n",
        "# Get the count of the specified word\n",
        "count = get_word_count(word)\n",
        "print(f\"The word '{word}' appears {count} times.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "o8znd8QJCpNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JkXStWnT0bj"
      },
      "outputs": [],
      "source": [
        "es_es_list = [\"Bueno\",\n",
        "    \"Por favor\",\n",
        "    \"Gracias\",\n",
        "    \"Lo siento\",\n",
        "    \"Me disculpa\",\n",
        "    \"¿Puedes?\",\n",
        "    \"¿Podrías?\",\n",
        "    \"¡Genial!\",\n",
        "    \"¡Muy bien hecho!\",\n",
        "    \"Esto es genial\",\n",
        "    \"¡Qué idea!\",\n",
        "    \"Qué bueno\",\n",
        "    \"Muy amable\",\n",
        "    \"¡Excelente trabajo!\",\n",
        "    \"Me parece muy bien\",\n",
        "    \"Estoy muy agradecido/a\",\n",
        "    \"Lo dejo en tus manos\",\n",
        "    \"¿Te importaría?\",\n",
        "    \"Por lo que más quieras\",\n",
        "    \"A tu disposición\"]\n",
        "\n",
        "freq_dist = trainset_es_es_1_pol_fd.most_common()\n",
        "\n",
        "for word in en_na_list:\n",
        "  word_frequency = dict(x).get(word, 0)\n",
        "  print(word, mama_frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Unjg4ZCdnl"
      },
      "source": [
        "# PART 6: Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aligned Word Vectors for English"
      ],
      "metadata": {
        "id": "Pw07bWY-EE7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXOplrH4dhkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67db6a2b-ed61-483c-dd1b-4edf77664a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 03:14:06--  https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.96, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5685446378 (5.3G) [binary/octet-stream]\n",
            "Saving to: ‘wiki.en.align.vec’\n",
            "\n",
            "wiki.en.align.vec   100%[===================>]   5.29G  52.7MB/s    in 1m 46s  \n",
            "\n",
            "2024-12-07 03:15:52 (51.1 MB/s) - ‘wiki.en.align.vec’ saved [5685446378/5685446378]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aligned Word Vectors for Spanish"
      ],
      "metadata": {
        "id": "Jgb1WNb_EDZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCgPw8lNqh23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba34ca1-2462-4ba8-978a-e98882ff6581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 03:15:54--  https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.14, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2227283009 (2.1G) [binary/octet-stream]\n",
            "Saving to: ‘wiki.es.align.vec’\n",
            "\n",
            "wiki.es.align.vec   100%[===================>]   2.07G  50.8MB/s    in 46s     \n",
            "\n",
            "2024-12-07 03:16:41 (46.3 MB/s) - ‘wiki.es.align.vec’ saved [2227283009/2227283009]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Million Most Frequent Types for English and Spanish\n",
        "\n",
        "(Adapted from provided class code on word vectors)"
      ],
      "metadata": {
        "id": "6boKDgNsEQLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROJ0iYiQtl64"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "english_vecs = {}\n",
        "spanish_vecs = {}\n",
        "\n",
        "# English Word Vectors\n",
        "count = 0\n",
        "with open('wiki.en.align.vec', 'r') as fp:\n",
        "  fp.readline()\n",
        "  for line in fp:\n",
        "    count += 1\n",
        "    if count > 1000000:\n",
        "      break\n",
        "    word, vector = line.split(' ', 1)\n",
        "    english_vecs[word] = np.fromstring(vector, sep = ' ')\n",
        "\n",
        "# Spanish Word Vectors\n",
        "count = 0\n",
        "with open('wiki.es.align.vec', 'r') as fp:\n",
        "  fp.readline()\n",
        "  for line in fp:\n",
        "    count += 1\n",
        "    if count > 1000000:\n",
        "      break\n",
        "    word, vector = line.split(' ', 1)\n",
        "    spanish_vecs[word] = np.fromstring(vector, sep = ' ')\n",
        "\n",
        "# Cosine Similarity\n",
        "def cosine_sim(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoMUNdUF24O1"
      },
      "source": [
        "### Top 20 Nearest Spanish Words Given an English Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeOgcRCdxYOP"
      },
      "outputs": [],
      "source": [
        "# One problem with the given class code is that phrases of words can not be considered\n",
        "# For example: nearest_spanish_word = nearest_es_from_en('thank you') would not have an output.\n",
        "# Partial solution was to consider the average/mean vector of the input (help from GPT-4o mini)\n",
        "\n",
        "def nearest_es_from_en(phrase, n = 20):\n",
        "    # Split the phrase into individual words\n",
        "    words = phrase.split()\n",
        "\n",
        "    # Retrieve the vectors for each word\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        vec = english_vecs.get(word)\n",
        "        if vec is not None:\n",
        "            vectors.append(vec)\n",
        "\n",
        "    # Check if at least one word was found\n",
        "    if not vectors:\n",
        "        print(\"Word(s) or phrase not found!\")\n",
        "        return\n",
        "\n",
        "    # Compute the mean/average vector of the phrase\n",
        "    phrase_vec = np.mean(vectors, axis = 0)\n",
        "\n",
        "    # Calculate similarities with all Spanish words\n",
        "    similarities = []\n",
        "    for spanish_word, spanish_vec in spanish_vecs.items():\n",
        "        similarities.append((spanish_word, cosine_sim(phrase_vec, spanish_vec)))\n",
        "\n",
        "    # Sort by similarity and get top N results\n",
        "    similarities.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "    if n > len(similarities):\n",
        "        n = len(similarities)\n",
        "\n",
        "    for w, sim in similarities[:n]:\n",
        "        print('%20s\\t%0.6f' % (w, sim))\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXsHzDtb-KRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c34c93f-602c-4035-80ba-cd568f583bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              deseas\t0.224552\n",
            "           procederé\t0.217111\n",
            "             ¿deseas\t0.216108\n",
            "                pido\t0.215288\n",
            "            solicito\t0.204628\n",
            "           ¿solicito\t0.204372\n",
            "          wikipágina\t0.204318\n",
            "nombre_de_wikiproyecto\t0.199628\n",
            "              puedes\t0.198190\n",
            "             pedirte\t0.195687\n",
            "            ¿procedo\t0.195214\n",
            "              ¿pongo\t0.195047\n",
            "             ¿puedes\t0.192317\n",
            "          agradeceré\t0.192043\n",
            "               ~~~~~\t0.191918\n",
            "              ¿puedo\t0.191907\n",
            "org/wiki/wikiproyecto\t0.189108\n",
            "          /wikipedia\t0.188956\n",
            "      anexo/artículo\t0.187547\n",
            "         wikiproyeto\t0.187461\n"
          ]
        }
      ],
      "source": [
        "nearest_spanish_word = nearest_es_from_en('please')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_spanish_word = nearest_es_from_en('if')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QjWBWU5QxEA",
        "outputId": "f58b3615-95b2-418a-f221-e1c1d28f9a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  si\t0.404058\n",
            "         ¿cualquiera\t0.324421\n",
            "                  no\t0.305505\n",
            "         ¡cualquiera\t0.303582\n",
            "          cualquiere\t0.294774\n",
            "          cualquiera\t0.288806\n",
            "      cualquiera—sin\t0.286237\n",
            "              ningún\t0.282333\n",
            "                 ¿es\t0.280328\n",
            "              porque\t0.279623\n",
            "          cualquiero\t0.278134\n",
            "             tampoco\t0.277424\n",
            "          ¿cualquier\t0.276977\n",
            "             ¿ningún\t0.276501\n",
            "         cualquieras\t0.275545\n",
            "             ningúno\t0.274245\n",
            "           supongase\t0.273346\n",
            "             #ningún\t0.271999\n",
            "      necesariamente\t0.266708\n",
            "          obviamente\t0.265972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2s-trpe7oKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8cb3d8-4af8-4f86-9f95-df706c0ce8f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             gracias\t0.409792\n",
            "            ¡gracias\t0.358676\n",
            "             ¡muchas\t0.321851\n",
            "         ¡muchísimas\t0.318771\n",
            "           ¡¡gracias\t0.317186\n",
            "            gracias,\t0.316728\n",
            "             ¡saludo\t0.312055\n",
            "            ,gracias\t0.307688\n",
            "          ¡¡¡gracias\t0.303359\n",
            "            ¡saludos\t0.303356\n",
            "            ¿gracias\t0.295662\n",
            "        enhorabuenas\t0.291111\n",
            "         enhorabuena\t0.286246\n",
            "          ¡excelente\t0.276907\n",
            "                   ۞\t0.273345\n",
            "         encarguemos\t0.272659\n",
            "          agradecere\t0.270600\n",
            "            人間はミスをする\t0.270469\n",
            "           agradeces\t0.270464\n",
            "            graciass\t0.269526\n"
          ]
        }
      ],
      "source": [
        "nearest_spanish_word = nearest_es_from_en('thanks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKcBmkUC7mVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720586bf-5734-4ca7-bb77-88571f46ce7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             pudiera\t0.414254\n",
            "               podía\t0.402941\n",
            "              podría\t0.395227\n",
            "             podrían\t0.390597\n",
            "            pudieran\t0.386168\n",
            "             pudiese\t0.385611\n",
            "              podían\t0.380994\n",
            "            pudiesen\t0.367089\n",
            "            pudieses\t0.328172\n",
            "              podido\t0.321217\n",
            "           puedieran\t0.318070\n",
            "            puediera\t0.316032\n",
            "            puediese\t0.311079\n",
            "            ¿pudiera\t0.308192\n",
            "     suficientemente\t0.304917\n",
            "           pudieseis\t0.301517\n",
            "            pudieron\t0.299595\n",
            "            pudieras\t0.297528\n",
            "               debía\t0.297259\n",
            "           puediesen\t0.294958\n"
          ]
        }
      ],
      "source": [
        "nearest_spanish_word = nearest_es_from_en('could')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRa2CcmN1NUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa081521-370d-4c0f-c731-edb2074225bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         agradecerme\t0.327418\n",
            "          perdonenme\t0.322565\n",
            "             ¿puedes\t0.309516\n",
            "         agradecerte\t0.309326\n",
            "             ¿tendré\t0.306774\n",
            "             ayudaré\t0.305585\n",
            "            ¡gracias\t0.304344\n",
            "            perdonen\t0.302163\n",
            "          agradeceré\t0.301066\n",
            "          agradecere\t0.300123\n",
            "              espero\t0.299671\n",
            "            olvidéis\t0.296243\n",
            "           agradecer\t0.295608\n",
            "           agradeces\t0.295125\n",
            "       agradeceremos\t0.294250\n",
            "             ¡saludo\t0.292881\n",
            "          ¿olvidaste\t0.292474\n",
            "            gustarte\t0.292308\n",
            "            ¿podrías\t0.291173\n",
            "         agradecemos\t0.291136\n"
          ]
        }
      ],
      "source": [
        "nearest_spanish_word = nearest_es_from_en('thank you')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52EMvHh_2_k0"
      },
      "source": [
        "### Top 20 Nearest English Words Given a Spanish Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPdL9uJEQ7pC"
      },
      "outputs": [],
      "source": [
        "def nearest_en_from_es(phrase, n = 20):\n",
        "    # Split the phrase into individual words\n",
        "    words = phrase.split()\n",
        "\n",
        "    # Retrieve the vectors for each word\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        vec = spanish_vecs.get(word)\n",
        "        if vec is not None:\n",
        "            vectors.append(vec)\n",
        "\n",
        "    # Check if at least one word was found\n",
        "    if not vectors:\n",
        "        print(\"Word(s) or phrase not found!\")\n",
        "        return\n",
        "\n",
        "    # Compute the mean/average vector of the phrase\n",
        "    phrase_vec = np.mean(vectors, axis = 0)\n",
        "\n",
        "    # Calculate similarities with all English words\n",
        "    similarities = []\n",
        "    for english_word, english_vec in english_vecs.items():\n",
        "        similarities.append((english_word, cosine_sim(phrase_vec, english_vec)))\n",
        "\n",
        "    # Sort by similarity and get top N results\n",
        "    similarities.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "    if n > len(similarities):\n",
        "        n = len(similarities)\n",
        "\n",
        "    for w, sim in similarities[:n]:\n",
        "        print('%20s\\t%0.6f' % (w, sim))\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66pRG0-g7fr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a986db4-e37f-4672-814d-5de6c7abce95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              thanks\t0.409792\n",
            "                help\t0.338952\n",
            "               thank\t0.296374\n",
            "             thanks,\t0.289152\n",
            "         appreciated\t0.279768\n",
            "              thanky\t0.279348\n",
            "     congratulations\t0.279304\n",
            "             #thanks\t0.278677\n",
            "                luck\t0.274287\n",
            "           excellent\t0.270585\n",
            "               kudos\t0.270372\n",
            "               thanx\t0.269991\n",
            "            thankyou\t0.268237\n",
            "              helps,\t0.267452\n",
            "               aided\t0.265716\n",
            "             enabled\t0.260650\n",
            "            congrats\t0.260345\n",
            "              helpin\t0.259798\n",
            "              hoping\t0.252104\n",
            "             support\t0.251582\n"
          ]
        }
      ],
      "source": [
        "nearest_english_word = nearest_en_from_es('gracias')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEYjXc3M7aEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afe100c-e556-4f30-ebc4-53fb8e6cff45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          appreciate\t0.339010\n",
            "               thank\t0.334544\n",
            "            thankyou\t0.299183\n",
            "         appreciated\t0.297816\n",
            "             commend\t0.279050\n",
            "           gratefull\t0.273407\n",
            "           apreciate\t0.269239\n",
            "             #thanks\t0.267875\n",
            "            request,\t0.267775\n",
            "            thankful\t0.262845\n",
            "     congratulations\t0.259238\n",
            "          gratefully\t0.256247\n",
            "             thankee\t0.256197\n",
            "             nichalp\t0.251431\n",
            "              thanks\t0.248946\n",
            "       help/guidance\t0.246179\n",
            "           enigmaman\t0.246070\n",
            "            hopefull\t0.245751\n",
            "              #thank\t0.244175\n",
            "             thanked\t0.241759\n"
          ]
        }
      ],
      "source": [
        "nearest_english_word = nearest_en_from_es('agradezco')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD4yvemc3Ywe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0752a635-c63c-4c5a-b007-782ad84c2e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  by\t0.413154\n",
            "             against\t0.300963\n",
            "               favor\t0.289233\n",
            "              favour\t0.262213\n",
            "                 for\t0.253873\n",
            "              behalf\t0.247317\n",
            "             opposed\t0.240706\n",
            "           supported\t0.235080\n",
            "            opposing\t0.230017\n",
            "             request\t0.220961\n",
            "           requested\t0.215520\n",
            "            support—\t0.214466\n",
            "            favoring\t0.202004\n",
            "            strongly\t0.200289\n",
            "            endorsed\t0.198308\n",
            "             approve\t0.198170\n",
            "                 and\t0.197975\n",
            "            rejected\t0.196991\n",
            "            proposal\t0.196939\n",
            "             reason,\t0.195458\n"
          ]
        }
      ],
      "source": [
        "nearest_english_word = nearest_en_from_es('por favor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5pFmkRhY70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791aa0b2-fdca-46d6-b5a2-3f8bf90db1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              thanks\t0.381843\n",
            "             thanks,\t0.325556\n",
            "               thank\t0.308200\n",
            "         appreciated\t0.298812\n",
            "                help\t0.294501\n",
            "              thanky\t0.281285\n",
            "     congratulations\t0.279367\n",
            "             #thanks\t0.276138\n",
            "                luck\t0.274732\n",
            "            thankyou\t0.269591\n",
            "               thanx\t0.267947\n",
            "           excellent\t0.264364\n",
            "               kudos\t0.262136\n",
            "             —thanks\t0.257135\n",
            "            congrats\t0.252153\n",
            "           wonderful\t0.250566\n",
            "             cheers,\t0.247747\n",
            "         assisstance\t0.246939\n",
            "                darn\t0.244791\n",
            "        tremendously\t0.242102\n"
          ]
        }
      ],
      "source": [
        "nearest_english_word = nearest_en_from_es('muchísimas gracias')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9WFHy1WTnir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af05953c-a357-49af-f64f-72aabaf63814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  by\t0.375464\n",
            "                 and\t0.265629\n",
            "                well\t0.262442\n",
            "                 all\t0.234586\n",
            "          everywhere\t0.233204\n",
            "                time\t0.229157\n",
            "                 for\t0.225046\n",
            "               again\t0.222236\n",
            "             greatly\t0.215379\n",
            "                once\t0.215077\n",
            "              sorely\t0.214713\n",
            "                rity\t0.211771\n",
            "             through\t0.211589\n",
            "                 way\t0.209211\n",
            "                help\t0.209185\n",
            "               which\t0.209030\n",
            "              lastly\t0.207109\n",
            "                much\t0.206615\n",
            "                hope\t0.206465\n",
            "                huge\t0.203218\n"
          ]
        }
      ],
      "source": [
        "nearest_english_word = nearest_en_from_es('gracias por todo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 7: Concordance Tables"
      ],
      "metadata": {
        "id": "ssc01BihDgPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# childes/Eng-NA/Bohannon/Bax/kelley\n",
        "devset_en_na_1_path = '/content/drive/Shareddrives/LIN 127 Project/Data Files/dev_childes _ Eng-NA _ Bohannon _ Bax _ kelley.csv'\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/dev_childes _ Eng-NA _ Bohannon _ Bax _ kelley.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    devset_en_na_1 = list(reader)\n",
        "\n",
        "# childes/Eng-UK/Edinburgh/adam1508\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/test_childes _ Eng-UK _ Edinburgh _ adam1508.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    testset_en_uk_1 = list(reader)"
      ],
      "metadata": {
        "id": "uT7SPzHxNc4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open(devset_en_na_1, 'r') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "\n",
        "    # Open a text file to write the utterances\n",
        "    with open('devset_concord.txt', 'w') as outfile:\n",
        "        for row in reader:\n",
        "            utterance = row['Utterance']  # Column that contains the utterances\n",
        "            outfile.write(utterance + '\\n')  # Write each utterance on a new line"
      ],
      "metadata": {
        "id": "SZpE1leWPZit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('devset_na_concord.txt', 'w') as outfile:\n",
        "    for row in devset_en_na_1:\n",
        "        # Replace 'utterance_column' with the actual column name in your CSV data\n",
        "        utterance = row[2]  # This extracts the utterance from the row\n",
        "        outfile.write(utterance + '\\n')  # Write each utterance on a new line\n"
      ],
      "metadata": {
        "id": "_Kw0aZ85KPn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRKDtBBUaj1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2867fa88-12c1-4191-cdae-3ac915199eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 6 of 6 matches:\n",
            "|6|ROOT 5|6|CASE 6|4|OBL 7|4|PUNCT see if he 'll go in the bucket . verb|see-Fin\n",
            "|2|NSUBJ 2|2|ROOT 3|2|PUNCT let 's see if you can catch . verb|let-Fin-Imp-S~pro\n",
            "|3|NSUBJ 3|3|ROOT 4|3|PUNCT whee , see if you can catch it . intj|whee cm|cm ver\n",
            " no . intj|no . 1|1|ROOT 2|1|PUNCT see if you can catch this . verb|see-Fin-Imp-\n",
            "BJ 5|5|ROOT 6|5|PUNCT xxx . let 's see if we can find it . verb|let-Fin-Imp-S~pr\n",
            "|DET 4|1|NSUBJ 5|1|PUNCT I do n't know if that one goes there , let 's see . pro\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#North American English Development Set\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "devset_en_na = 'devset_concord.txt'\n",
        "dev_en_na_file = open('devset_na_concord.txt')\n",
        "dev_en_na_text = dev_en_na_file.read()\n",
        "dev_en_na_file.close()\n",
        "dev_en_na_tokens = nltk.word_tokenize(dev_en_na_text)\n",
        "\n",
        "dev_en_na = nltk.Text(dev_en_na_tokens)\n",
        "dev_en_na.concordance('if', 80, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('testset_uk_concord.txt', 'w') as outfile:\n",
        "    for row in testset_en_uk_1:\n",
        "        # Replace 'utterance_column' with the actual column name in your CSV data\n",
        "        utterance = row[2]  # This extracts the utterance from the row\n",
        "        outfile.write(utterance + '\\n')  # Write each utterance on a new line"
      ],
      "metadata": {
        "id": "lZ6QV7LwQ9Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09WVvUCvbZef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d08884b-490c-463d-d4b9-5a9a9d6eed60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            " 1|3|ROOT 2|1|OBJ 3|1|ADVMOD 4|1|PUNCT if I do that there , then I can + ... sco\n",
            "uld take you to the library more often if it 's gon na [ going to ] make you sle\n",
            " I do n't think that was I 'm not sure if that was it but + ... [ + NAC ] pron|I\n"
          ]
        }
      ],
      "source": [
        "#UK English Test Set\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "test_en_uk_file = open('testset_uk_concord.txt')\n",
        "test_en_uk_text = test_en_uk_file.read()\n",
        "test_en_uk_file.close()\n",
        "test_en_uk_tokens = nltk.word_tokenize(test_en_uk_text)\n",
        "\n",
        "test_en_uk = nltk.Text(test_en_uk_tokens)\n",
        "test_en_uk.concordance('if', 80, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# childes/Spanish/Montes/020914\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/train_childes_Spanish(México)_Montes_020914.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    trainset_es_mx_2 = list(reader)\n",
        "\n",
        "# childes/Spanish/Marrero/Rafael/040416\n",
        "with open('/content/drive/Shareddrives/LIN 127 Project/Data Files/test_childes_Spanish(Spain)_marrero_rafael_040416.csv') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    testset_es_es_1 = list(reader)"
      ],
      "metadata": {
        "id": "8k8bieWSRuWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('trainset_es_mx_2concord.txt', 'w') as outfile:\n",
        "    for row in trainset_es_mx_2:\n",
        "        # Replace 'utterance_column' with the actual column name in your CSV data\n",
        "        utterance = row[1]  # This extracts the utterance from the row\n",
        "        outfile.write(utterance + '\\n')  # Write each utterance on a new line"
      ],
      "metadata": {
        "id": "WpypTF8HR61y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EYkovJ-bgpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4e7ff9-906a-47fc-ec03-054064211c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no matches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Mexican Spanish Development Set\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "trainset_es_mx_2_file = open('trainset_es_mx_concord.txt')\n",
        "trainset_es_mx_2_text = trainset_es_mx_2_file.read()\n",
        "trainset_es_mx_2_file.close()\n",
        "trainset_es_mx_2_tokens = nltk.word_tokenize(trainset_es_mx_2_text)\n",
        "\n",
        "trainset_es_mx_2 = nltk.Text(trainset_es_mx_2_tokens)\n",
        "trainset_es_mx_2.concordance('si', 80, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('testset_es_es_1_concord.txt', 'w') as outfile:\n",
        "    for row in testset_es_es_1:\n",
        "        # Replace 'utterance_column' with the actual column name in your CSV data\n",
        "        utterance = row[1]  # This extracts the utterance from the row\n",
        "        outfile.write(utterance + '\\n')  # Write each utterance on a new line"
      ],
      "metadata": {
        "id": "cENhu2pFSuMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIh0LR00cNQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259cae4e-d124-40ea-92d4-0a3a44f7ca03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 29 of 29 matches:\n",
            "MJO : qué pasa ? CHI : que [ // ] pero si ahora es la hora de merendar . MJO : e\n",
            " . MJO : vale , haha vale . CHI : pero si es la hora de darle el topetazo [ * ] \n",
            "es [ * ] ? MJO : no me acuerdo , no sé si los he metido o no , si no los metemos\n",
            "cuerdo , no sé si los he metido o no , si no los metemos en la otra piñata , val\n",
            " [ // ] cayó en ésta ? CHI : ay , pero si hay + ... MJO : hala , si hay otro pis\n",
            " : ay , pero si hay + ... MJO : hala , si hay otro piso debajo [ = en la caja de\n",
            "tas para que vengan ? CHI : mhm , pero si se tropiezan [ * ] con todo esto [ = l\n",
            "ído de la piñata ] ? MJO : qué ? CHI : si se tropiezan [ * ] ? MJO : con los hij\n",
            "ntonces cómo hacemos ? CHI : sabes que si abrimos esto [ = un agujero imaginario\n",
            " hacia aquí vienen ? CHI : sí , es que si abres esto vienen todas las latas ! MJ\n",
            "y , ay , ay , que se [ / ] que se caen si abres ! MJO : ya , ya está , ya está c\n",
            " está , ya está cerrado . CHI : porque si no & ~no [ / ] nosotros nos quedaremos\n",
            "] cae la azúcar [ * ] , cae la leche y si le das [ // ] y si le doy al [ / ] al \n",
            " ] , cae la leche y si le das [ // ] y si le doy al [ / ] al botón de azúcar [ *\n",
            "está el vaso ? CHI : chic @ o , es que si lo pones aquí el vaso sale . MJO : ah \n",
            "acacas falta . CHI : falta [ // ] pero si a Patacacas le he dado , hombre . MJO \n",
            " aquí está . CHI : oye , pero hombre , si [ / ] si tiene que ir en la máquina , \n",
            "á . CHI : oye , pero hombre , si [ / ] si tiene que ir en la máquina , tienes qu\n",
            " pues papá te da más . CHI : ay , pero si falta éste también . MJO : falta este \n",
            "os a algo con los hijitos , eh ? MJO : si ya estamos jugando con ellos . CHI : a\n",
            "n bebidas . CHI : pues no . CHI : pero si ésta es la siesta [ : fiesta ] [ * ] ,\n",
            "ee ? MJO : espera , date la vuelta que si no no te ven los hijitos . MJO : 0 [ =\n",
            " MJO : a las adivinanzas . MJO : a_ver si adivinamos lo que dice Rafita . ALF : \n",
            "a & ~e Jirafón . MJO : Jirafón , a_ver si se gana algo . CHI : 0 [ = ! esconde u\n",
            "MJO : pero entonces lo va a adivinar , si ya sabe que es Jirafón lo va a adivina\n",
            "JO : sabes ? MJO : ahora podemos jugar si quieres , como en el cole , a hacer un\n",
            "er , yo te voy a contar una cosa a_ver si adivinas quién soy . MJO : quién ha ha\n",
            "bón , Dios mío ! CHI : Dios [ ! ] pero si era de luego . MJO : era de uno de ell\n",
            " sacar esto que ha metido Alfon porque si no no se puede . CHI : qué ha & ~metí \n"
          ]
        }
      ],
      "source": [
        "#Castilian Spanish Test Set\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "testset_es_es_1_file = open('testset_es_es_1_concord.txt')\n",
        "testset_es_es_1_text = testset_es_es_1_file.read()\n",
        "testset_es_es_1_file.close()\n",
        "testset_es_es_1_tokens = nltk.word_tokenize(testset_es_es_1_text)\n",
        "\n",
        "testset_es_es_1 = nltk.Text(testset_es_es_1_tokens)\n",
        "testset_es_es_1.concordance('si', 80, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unused Word Vector Code"
      ],
      "metadata": {
        "id": "BKsu13DpDBln"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM_R5CzBEEgf"
      },
      "source": [
        "See code below for attempt to get a phrase of words as output given an input\n",
        "\n",
        "Given a phrase of words like \"thank you\",\n",
        "```\n",
        "nearest_es_from_en(\"thank you\", n=5, max_output_length=3)\n",
        "```\n",
        " was hoping/trying to get an output like the following where a defined window of words retained the meaning of the input.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "              gracias\t0.905432\n",
        "      muchas gracias\t0.895123\n",
        "      te agradezco mucho\t0.883451\n",
        "    muchísimas gracias\t0.876532\n",
        "       gracias por todo\t0.874230\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tqK1-P7QHZb"
      },
      "outputs": [],
      "source": [
        "# Nearest Spanish word given an English word\n",
        "# unused code v1.1\n",
        "\n",
        "def nearest_es_from_en(word, n = 20):\n",
        "  english_vec = english_vecs.get(word)\n",
        "\n",
        "  if english_vec is None:\n",
        "    print('Word not found!')\n",
        "    return\n",
        "\n",
        "  similarities = []\n",
        "  for spanish_word, spanish_vec in spanish_vecs.items():\n",
        "    similarities.append((spanish_word, cosine_sim(english_vec, spanish_vec)))\n",
        "\n",
        "  similarities.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "  if n > len(similarities):\n",
        "    n = len(similarities)\n",
        "\n",
        "  for w, sim in similarities[:n]:\n",
        "    print('%20s\\t%0.6f' % (w, sim))\n",
        "\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP8NUmKq20mP"
      },
      "outputs": [],
      "source": [
        "# Nearest English word given a Spanish word\n",
        "# unused code v1.2\n",
        "\n",
        "def nearest_en_from_es(word, n = 20):\n",
        "  spanish_vec = spanish_vecs.get(word)\n",
        "\n",
        "  if spanish_vec is None:\n",
        "    print('Word not found!')\n",
        "    return\n",
        "\n",
        "  similarities = []\n",
        "  for english_word, english_vec in english_vecs.items():\n",
        "    similarities.append((english_word, cosine_sim(spanish_vec, english_vec)))\n",
        "\n",
        "  similarities.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "  if n > len(similarities):\n",
        "    n = len(similarities)\n",
        "\n",
        "  for w, sim in similarities[:n]:\n",
        "    print('%20s\\t%0.6f' % (w, sim))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUmFr2Pa_FGR"
      },
      "outputs": [],
      "source": [
        "# Created with chatgpt v2 (unused code)\n",
        "# Attempt to get a phrase of words as output using the word vectors when given a phrase of words\n",
        "\n",
        "def nearest_es_from_en(phrase, n=20, window=5):\n",
        "    \"\"\"\n",
        "    Find the nearest Spanish words for an English phrase.\n",
        "\n",
        "    Args:\n",
        "        phrase (str): The input English phrase.\n",
        "        n (int): Number of top results to compute before applying the window.\n",
        "        window (int): Number of words in the output window.\n",
        "    \"\"\"\n",
        "    # Split the phrase into individual words\n",
        "    words = phrase.split()\n",
        "\n",
        "    # Retrieve the vectors for each word\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        vec = english_vecs.get(word)\n",
        "        if vec is not None:\n",
        "            vectors.append(vec)\n",
        "\n",
        "    # Check if at least one word was found\n",
        "    if not vectors:\n",
        "        print(\"Word(s) not found!\")\n",
        "        return\n",
        "\n",
        "    # Compute the mean vector of the phrase\n",
        "    phrase_vec = np.mean(vectors, axis=0)\n",
        "\n",
        "    # Calculate similarities with all Spanish words\n",
        "    similarities = []\n",
        "    for spanish_word, spanish_vec in spanish_vecs.items():\n",
        "        similarities.append((spanish_word, cosine_sim(phrase_vec, spanish_vec)))\n",
        "\n",
        "    # Sort by similarity\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Apply window size\n",
        "    if window > len(similarities):\n",
        "        window = len(similarities)\n",
        "\n",
        "    # Display the top 'window' results\n",
        "    for w, sim in similarities[:window]:\n",
        "        print('%20s\\t%0.6f' % (w, sim))\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrgMjRLIAslI"
      },
      "outputs": [],
      "source": [
        "# Created with chatgpt v3 (unused code)\n",
        "# Attempt to get a phrase of words as output using the word vectors when given a phrase of words\n",
        "\n",
        "def nearest_es_from_en(phrase, n=20, max_output_length=3):\n",
        "    # Split the input phrase into individual words\n",
        "    words = phrase.split()\n",
        "\n",
        "    # Retrieve the vectors for each word\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        vec = english_vecs.get(word)\n",
        "        if vec is not None:\n",
        "            vectors.append(vec)\n",
        "\n",
        "    # Check if at least one word was found\n",
        "    if not vectors:\n",
        "        print(\"Word(s) not found!\")\n",
        "        return\n",
        "\n",
        "    # Compute the mean vector of the phrase\n",
        "    phrase_vec = np.mean(vectors, axis=0)\n",
        "\n",
        "    # Calculate similarities with all Spanish words\n",
        "    similarities = []\n",
        "    for spanish_word, spanish_vec in spanish_vecs.items():\n",
        "        # Count the number of words in the Spanish word/phrase\n",
        "        word_count = len(spanish_word.split('_'))\n",
        "\n",
        "        # Only include Spanish words/phrases within the max_output_length constraint\n",
        "        if word_count <= max_output_length:\n",
        "            similarities.append((spanish_word, cosine_sim(phrase_vec, spanish_vec)))\n",
        "\n",
        "    # Sort by similarity and get top N results\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if n > len(similarities):\n",
        "        n = len(similarities)\n",
        "\n",
        "    for w, sim in similarities[:n]:\n",
        "        print('%20s\\t%0.6f' % (w.replace('_', ' '), sim))  # Replace underscores with spaces for readability\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Q3RwzfA4Zl"
      },
      "outputs": [],
      "source": [
        "nearest_es_from_en(\"thank you so much\", n=50, max_output_length=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The End. Thank you!"
      ],
      "metadata": {
        "id": "7bQT6DVp2g87"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "BKsu13DpDBln"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}